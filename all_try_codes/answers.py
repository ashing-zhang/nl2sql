import json
import sqlite3
import re
import os
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    pipeline,
    StoppingCriteriaList
)
from modelscope import AutoTokenizer, AutoModelForCausalLM
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from stop_criteria_utils import MaxLengthStopCriteria, RepetitionStopCriteria, LowConfidenceStopCriteria
from concurrent.futures import ThreadPoolExecutor

# ÁªÑÂêàÂ§ö‰∏™ÁªàÊ≠¢Êù°‰ª∂
stop_criteria_list = StoppingCriteriaList([
    MaxLengthStopCriteria(200),  # ÊúÄÈïø 200 ‰∏™ token
    RepetitionStopCriteria(3),   # ÈÅøÂÖç 3 ËøûÈáçÂ§ç
    LowConfidenceStopCriteria(0.1)  # ÊúÄÈ´òÁΩÆ‰ø°Â∫¶‰Ωé‰∫é 0.1 Êó∂ÂÅúÊ≠¢
])

class FinancialQA:
    def __init__(self, config):
        # ÂàùÂßãÂåñËÆæÂ§áÈÖçÁΩÆ
        self.device = config["device"]
        self.is_cuda = "cuda" in self.device
        self.config = config
        
        # ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìËøûÊé•
        self.conn = sqlite3.connect(config["db_path"])
        self.db_schema = json.load(open(config["db_schema_path"], "r", encoding="utf-8"))
        self.all_columns = [col["name"] for table in self.db_schema.values() for col in table]
        with open(config["db_keywords_path"], "r", encoding="utf-8") as f:
            self.db_keywords = [line.strip() for line in f if line.strip()]
        # ÁîüÊàêË°®ÁªìÊûÑÊèèËø∞
        self.table_descs = []
        for table_name, columns in self.db_schema.items():
            col_details = "\n".join([f"    ‚ñ™ {col['name']} ({col['type']})" for col in columns])
            self.table_descs.append(f"‚ñå Ë°®ÂêçÔºö{table_name}\n{col_details}")
        # ‰ªérules.txtÊñá‰ª∂‰∏≠Âä†ËΩΩËΩ¨Êç¢ËßÑÂàô
        with open('rules.txt', 'r', encoding='utf-8') as f:
            self.rules = f.read()
        
        # ‰ªédata/example_queries.txtÊñá‰ª∂‰∏≠Âä†ËΩΩ‰∏öÂä°ÁâπÂÆöÁ§∫‰æã
        self.example_queries = []
        with open(self.config["example_queries_path"], 'r', encoding='utf-8') as f:
            for line in f:
                question, sql = line.strip().split(", ", 1)
                self.example_queries.append(('Ëá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢Ôºö'+ question, 'ÂØπÂ∫îÁöÑsqlÊü•ËØ¢Ôºö' + sql))
        print('self.example_queries[0]:',self.example_queries[0])
        # ÂàùÂßãÂåñÊñáÊú¨ÁîüÊàêÊ®°Âûã
        self.text_gen_tokenizer = AutoTokenizer.from_pretrained(
            config["text_gen_model"], 
            trust_remote_code=True
        )
        self.text_gen_model = AutoModelForCausalLM.from_pretrained(
            config["text_gen_model"],
            torch_dtype=torch.float16 if self.is_cuda else torch.float32,
        ).to(self.device)
        
        self.text_gen_pipe = pipeline(
            "text-generation",
            model=self.text_gen_model,
            tokenizer=self.text_gen_tokenizer,
            device=self.device
        )
        
        # ÂàùÂßãÂåñText-to-SQLÊ®°ÂûãÔºà‰∏éÊñáÊú¨ÁîüÊàêÊ®°ÂûãÁõ∏ÂêåÔºâ
        self.sql_tokenizer = self.text_gen_tokenizer
        self.sql_model = self.text_gen_model

        # ÂàùÂßãÂåñRAGÁ≥ªÁªü
        self.embeddings = HuggingFaceEmbeddings(
            model_name=config["embed_model"],
            model_kwargs={"device": self.device.split(":")[0], "local_files_only": False},
            encode_kwargs={"normalize_embeddings": False}
        )
        persist_dir = "data/chroma_db"+"/"+config["embed_model"]
        self.vector_store = self._init_vector_store(config["txt_dir"], persist_dir)
    

    def _generate_sql_prompt(self, question):
        return f"""
            üìå ‰ªªÂä°ÊèèËø∞Ôºö
            ‰Ω†ÊòØ‰∏ÄÂêç SQL ÁîüÊàê‰∏ìÂÆ∂ÔºåÈúÄË¶ÅÂ∞ÜÁî®Êà∑ÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢òËΩ¨Êç¢‰∏∫ **È´òÊïà‰∏îÂáÜÁ°ÆÁöÑ SQL Êü•ËØ¢**„ÄÇ
            
            üìä **Êï∞ÊçÆÂ∫ìÊ®°ÂºèÔºàSchemaÔºâ**Ôºö
            {'\n\n'.join(self.table_descs)}

            üìù **SQL ÁîüÊàêË¶ÅÊ±ÇÔºö**
            {self.rules}

            üõ†Ô∏è Ê†πÊçÆ‰ª•‰∏ä‰ø°ÊÅØÔºàÂåÖÊã¨Êï∞ÊçÆÂ∫ìÊ®°Âºè„ÄÅSQLÁîüÊàêË¶ÅÊ±ÇÁ≠âÔºâÔºå
            ËØ∑Â∞Ü‰ª•‰∏ãÈóÆÈ¢òËΩ¨Êç¢‰∏∫SQLËØ≠Âè•Ôºö
            {question}
            üéØ **ÊúÄÁªà SQL Êü•ËØ¢**Ôºö
            
            """.strip()

    def _sanitize_sql(self, sql):
        sql = sql.strip().replace("```sql", "").replace("```", "")  # Ê∏ÖÁêÜÊ†áËÆ∞
        # print('sql:', sql)
        
        # ÊâæÂá∫sql‰∏≠ÁöÑÊâÄÊúâÂ≠óÊÆµ
        # sql_columns = []
        # for column in self.all_columns:
        #     if column in sql:
        #         sql_columns.append(column)
        
        # print('sql after sanitize:', sql)
        return sql

    def _execute_sql(self, question, sql):
        try:
            cursor = self.conn.cursor()
            cursor.execute(sql)
            result = cursor.fetchall()
            
            # Combine question and result to generate a natural language answer
            prompt = f"ÈóÆÈ¢òÔºö{question}\nSQLÊü•ËØ¢ÁªìÊûúÔºö{result}\nËØ∑Ê†πÊçÆÊü•ËØ¢ÁªìÊûúÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÁ≠îÊ°àÔºà‰ªÖÁªôÂá∫Á≠îÊ°àÂç≥ÂèØÔºåÊó†ÈúÄÈáçÂ§çÈóÆÈ¢òÔºâÔºö"
            inputs = self.text_gen_tokenizer(prompt, return_tensors="pt").to(self.device)
            generated = self.text_gen_model.generate(
                inputs["input_ids"],
                max_new_tokens=150,
                early_stopping=True
            )
            answer = self.text_gen_tokenizer.decode(generated[0], skip_special_tokens=True)
            return answer.strip()
        except sqlite3.Error as e:
            print(f"[SQL Error] {str(e)}")
            # Êä•ÈîôÊó∂Â≠òÂÇ®ÈóÆÈ¢ò„ÄÅSQLÂíåÈîôËØØ‰ø°ÊÅØ
            with open('sql_log.json', 'a', encoding='utf-8') as log_file:
                json.dump(
                    {"question": question, "sql": sql, "error": str(e)},
                    log_file,
                    ensure_ascii=False,
                    indent=4
                )
                log_file.write("\n")
            return None

    def _init_vector_store(self, txt_dir, persist_dir):
        if os.path.exists(persist_dir):
            print(f"ÂêëÈáèÊï∞ÊçÆÂ∫ìÂ∑≤Â≠òÂú®‰∫é {persist_dir}ÔºåË∑≥ËøáÂàùÂßãÂåñ")
            return Chroma(persist_directory=persist_dir, embedding_function=self.embeddings)
        
        docs = []
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100
        )
        
        for filename in tqdm(os.listdir(txt_dir), desc="Âä†ËΩΩÊñáÊ°£"):
            if filename.endswith(".txt"):
                with open(os.path.join(txt_dir, filename), "r", encoding="utf-8") as f:
                    text = f.read()
                docs.extend(splitter.create_documents([text]))
        
        return Chroma.from_documents(
            documents=docs,
            embedding=self.embeddings,
            persist_directory=persist_dir
        )

    def classify_task(self, questions):
        # ÂéªÈô§Ê†áÁÇπÁ¨¶Âè∑Ôºå‰ªéËÄå‰∏çÂπ≤Êâ∞question_vectorsÁöÑÁîüÊàêÔºåËøõËÄå‰∏çÂΩ±ÂìçÁõ∏‰ººÂ∫¶ËÆ°ÁÆó
        questions = ["".join(c for c in q if c not in "?!ÔºüÔºÅÔºå„ÄÇ") for q in questions]
        def determine_task_type(question):
            # Á°Æ‰øùÁºñÁ†ÅÊ≠£Á°Æ
            question = question.encode("utf-8").decode("utf-8")

            # ‰ΩøÁî® encode() + decode() ÊñπÂºèËøõË°åÂàÜËØç
            token_ids = self.text_gen_tokenizer.encode(question, add_special_tokens=False)
            # question_words = set(self.text_gen_tokenizer.decode([token]) for token in token_ids)
            '''
            ÂàÜËØçÁªìÊûúÁ§∫‰æãÔºö
                question_words: {'Âπ¥', 'Âü∫Èáë', 'Âòâ', '?', 'ÂÆû', 'Âü∫ÈáëÁÆ°ÁêÜ', 'ÊúâÈôêÂÖ¨Âè∏', '0', '2', 'ÊàêÁ´ã‰∫Ü', '9', 'Â§öÂ∞ë', '1'}
                question_words: {'ÊäÄÊúØ', 'ËÇ°‰ªΩ', 'Ë¥üË¥£', 'ÁîüÁâ©', 'ÊúâÈôêÂÖ¨Âè∏', 'Ôºü', '‰∫ßÂìÅÁ†îÂèë', 'Ê£Æ', 'Ê≤É', '‰∫ëÂçó', '‰ªÄ‰πà', 'ÈÉ®Èó®', 'ÁöÑÊòØ'}
            '''
            # print("question_words:", question_words)

            match_count = sum(1 for char in question if any(char in keyword for keyword in self.db_keywords))
            if match_count / len(question) >= 0.3:
                return "data_query"
            else:
                docs = self.vector_store.similarity_search(question, k=1)
                return "text_comprehension" if docs else "other"

        return [determine_task_type(question) for question in questions]

    def structured_query(self, question):
        sql_prompt = self._generate_sql_prompt(question)
        # print('question:', question)
        # print('sql_prompt:', sql_prompt)
        inputs = self.sql_tokenizer(sql_prompt, return_tensors="pt").to(self.device)
        
        generated = self.sql_model.generate(
            inputs["input_ids"],
            max_new_tokens=500,
            num_return_sequences=3,
            early_stopping=True
        )
        sql = self.sql_tokenizer.decode(generated[0], skip_special_tokens=True)
        sql = sql.split("```sql\n")[-1].split(";")[0] + ";"
        # print('sql:', sql)
        
        # sql = self._sanitize_sql(sql)
        
        # Store the question and corresponding SQL in a file
        with open('question_to_sql.json', 'a', encoding='utf-8') as f:
            json.dump({"question": question, "sql": sql}, f, ensure_ascii=False, indent=4)
            f.write("\n")
        
        return self._execute_sql(question,sql)
        
    def rag_answer(self, questions):
        contexts = []
        for question in questions:
            docs = self.vector_store.similarity_search(question, k=3)
            context = "\n".join([d.page_content for d in docs])
            contexts.append(context)

        prompts = [f"""Âü∫‰∫é‰ª•‰∏ã‰ø°ÊÅØÂõûÁ≠îÈóÆÈ¢òÔºö
                    {context}

                    ÈóÆÈ¢òÔºö{question}
                    Á≠îÊ°àÔºö""" for context, question in zip(contexts, questions)]
        
        responses = self.text_gen_pipe(prompts, max_new_tokens=300, do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.2, stopping_criteria=stop_criteria_list)

        return [response[0]["generated_text"].split("Á≠îÊ°àÔºö")[-1].strip() for response in responses]

    def process_batch(self, qid_batch, question_batch):
        task_types = self.classify_task(question_batch)
        answers = []
        
        with open('question_type.json', 'a', encoding='utf-8') as f:
            for task_type, question in zip(task_types, question_batch):
                json.dump({"question": question, "task_type": task_type}, f, ensure_ascii=False, indent=4)
                f.write("\n")
                answer = "No valid answer found"  # Default initialization
                if task_type == "data_query":
                    result = self.structured_query(question)
                    # print('question:', question)
                    # print('result:', result)
                    if result:
                        answer = " ".join([str(r) for r in result])
                else:
                    answer = self.rag_answer([question])[0]
                
                answers.append(answer)
        
        return [{"id": qid, "question": question, "answer": answer} for qid, question, answer in zip(qid_batch, question_batch, answers)]


class QuestionDataset(Dataset):
    def __init__(self, data_path):
        self.questions = []
        with open(data_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    self.questions.append(json.loads(line))

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        return self.questions[idx]

# ‰ΩøÁî®Á§∫‰æã
if __name__ == "__main__":
    # Ê∏ÖÈô§question_to_sql.json‰∏≠ÁöÑÂÜÖÂÆπ
    with open('question_to_sql.json', 'w', encoding='utf-8') as f:
        f.write("")
    # Ê∏ÖÈô§sql_log.json‰∏≠ÁöÑÂÜÖÂÆπ
    with open('sql_log.json', 'w', encoding='utf-8') as f:
        f.write("")
    # Ê∏ÖÈô§question_type.json‰∏≠ÁöÑÂÜÖÂÆπ
    with open('question_type.json', 'w', encoding='utf-8') as f:
        f.write("")
    
    config = {
        # "text_gen_model": "models/Qwen2.5-7B-Instruct",
        # "text2sql_model": "models/Qwen2.5-7B-Instruct",
        "text_gen_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "text2sql_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "embed_model": "models/all-MiniLM-L6-v2",
        "db_path": "data/dataset/fund_data.db",
        "db_schema_path": "data/dataset/db_schema.json",
        "example_queries_path": "data/example_queries.txt",
        "txt_dir": "data/pdf_txt_file",
        "db_keywords_path": "data/dataset/db_keywords.txt",
        "device": "cuda:0" if torch.cuda.is_available() else "cpu"
    }
    
    qa_system = FinancialQA(config)
    
    data_path = "data/question_debug.json"
    # data_path = "data/question.json"
    dataset = QuestionDataset(data_path)
    dataloader = DataLoader(dataset, batch_size=5, shuffle=False)

    # ‰ΩøÁî®ÊâπÈáèÊé®ÁêÜ
    results = []
    for batch in tqdm(dataloader, desc="Processing questions"):
        qid_batch = batch['id'].tolist()
        question_batch = batch['question']
        results.extend(qa_system.process_batch(qid_batch, question_batch))

    # Ensure all tensors are converted to lists before writing to JSON
    def tensor_to_list(obj):
        if isinstance(obj, torch.Tensor):
            return obj.tolist()
        if isinstance(obj, list):
            return [tensor_to_list(item) for item in obj]
        if isinstance(obj, dict):
            return {key: tensor_to_list(value) for key, value in obj.items()}
        return obj

    results = tensor_to_list(results)
    # print('results:', results)
    with open("submit_result.jsonl", "w", encoding="utf-8") as f:
        for res in results:
            f.write(json.dumps(res, ensure_ascii=False) + "\n")
    
    print(f"Processed {len(results)} questions.")
