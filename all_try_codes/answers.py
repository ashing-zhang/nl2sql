import json
import sqlite3
import re
import os
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    pipeline,
    StoppingCriteriaList
)
from modelscope import AutoTokenizer, AutoModelForCausalLM
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from stop_criteria_utils import MaxLengthStopCriteria, RepetitionStopCriteria, LowConfidenceStopCriteria
from concurrent.futures import ThreadPoolExecutor

# ç»„åˆå¤šä¸ªç»ˆæ­¢æ¡ä»¶
stop_criteria_list = StoppingCriteriaList([
    MaxLengthStopCriteria(200),  # æœ€é•¿ 200 ä¸ª token
    RepetitionStopCriteria(3),   # é¿å… 3 è¿é‡å¤
    LowConfidenceStopCriteria(0.1)  # æœ€é«˜ç½®ä¿¡åº¦ä½äº 0.1 æ—¶åœæ­¢
])

class FinancialQA:
    def __init__(self, config):
        # åˆå§‹åŒ–è®¾å¤‡é…ç½®
        self.device = config["device"]
        self.is_cuda = "cuda" in self.device
        self.config = config
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self.conn = sqlite3.connect(config["db_path"])
        self.db_schema = json.load(open(config["db_schema_path"], "r", encoding="utf-8"))
        self.all_columns = [col["name"] for table in self.db_schema.values() for col in table]
        with open(config["db_keywords_path"], "r", encoding="utf-8") as f:
            self.db_keywords = [line.strip() for line in f if line.strip()]
        # ç”Ÿæˆè¡¨ç»“æ„æè¿°
        self.table_descs = []
        for table_name, columns in self.db_schema.items():
            col_details = "\n".join([f"    â–ª {col['name']} ({col['type']})" for col in columns])
            self.table_descs.append(f"â–Œ è¡¨åï¼š{table_name}\n{col_details}")
        # ä»rules.txtæ–‡ä»¶ä¸­åŠ è½½è½¬æ¢è§„åˆ™
        with open('rules.txt', 'r', encoding='utf-8') as f:
            self.rules = f.read()
        
        # ä»data/example_queries.txtæ–‡ä»¶ä¸­åŠ è½½ä¸šåŠ¡ç‰¹å®šç¤ºä¾‹
        self.example_queries = []
        with open(self.config["example_queries_path"], 'r', encoding='utf-8') as f:
            for line in f:
                question, sql = line.strip().split(", ", 1)
                self.example_queries.append(('è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼š'+ question, 'å¯¹åº”çš„sqlæŸ¥è¯¢ï¼š' + sql))
        print('self.example_queries[0]:',self.example_queries[0])
        # åˆå§‹åŒ–æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
        self.text_gen_tokenizer = AutoTokenizer.from_pretrained(
            config["text_gen_model"], 
            trust_remote_code=True
        )
        self.text_gen_model = AutoModelForCausalLM.from_pretrained(
            config["text_gen_model"],
            torch_dtype=torch.float16 if self.is_cuda else torch.float32,
        ).to(self.device)
        
        self.text_gen_pipe = pipeline(
            "text-generation",
            model=self.text_gen_model,
            tokenizer=self.text_gen_tokenizer,
            device=self.device
        )
        
        # åˆå§‹åŒ–Text-to-SQLæ¨¡å‹ï¼ˆä¸æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ç›¸åŒï¼‰
        self.sql_tokenizer = self.text_gen_tokenizer
        self.sql_model = self.text_gen_model

        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        self.embeddings = HuggingFaceEmbeddings(
            model_name=config["embed_model"],
            model_kwargs={"device": self.device.split(":")[0], "local_files_only": False},
            encode_kwargs={"normalize_embeddings": False}
        )
        persist_dir = "data/chroma_db"+"/"+config["embed_model"]
        self.vector_store = self._init_vector_store(config["txt_dir"], persist_dir)
    

    def _generate_sql_prompt(self, question):
        return f"""
            ğŸ“Œ ä»»åŠ¡æè¿°ï¼š
            ä½ æ˜¯ä¸€å SQL ç”Ÿæˆä¸“å®¶ï¼Œéœ€è¦å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸º **é«˜æ•ˆä¸”å‡†ç¡®çš„ SQL æŸ¥è¯¢**ã€‚
            
            ğŸ“Š **æ•°æ®åº“æ¨¡å¼ï¼ˆSchemaï¼‰**ï¼š
            {'\n\n'.join(self.table_descs)}

            ğŸ“ **SQL ç”Ÿæˆè¦æ±‚ï¼š**
            {self.rules}

            ğŸ› ï¸ æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼ˆåŒ…æ‹¬æ•°æ®åº“æ¨¡å¼ã€SQLç”Ÿæˆè¦æ±‚ç­‰ï¼‰ï¼Œ
            è¯·å°†ä»¥ä¸‹é—®é¢˜è½¬æ¢ä¸ºSQLè¯­å¥ï¼š
            {question}
            ğŸ¯ **æœ€ç»ˆ SQL æŸ¥è¯¢**ï¼š
            
            """.strip()

    def _sanitize_sql(self, sql):
        sql = sql.strip().replace("```sql", "").replace("```", "")  # æ¸…ç†æ ‡è®°
        # print('sql:', sql)
        
        # æ‰¾å‡ºsqlä¸­çš„æ‰€æœ‰å­—æ®µ
        # sql_columns = []
        # for column in self.all_columns:
        #     if column in sql:
        #         sql_columns.append(column)
        
        # print('sql after sanitize:', sql)
        return sql

    def _execute_sql(self, question, sql):
        try:
            cursor = self.conn.cursor()
            cursor.execute(sql)
            result = cursor.fetchall()
            
            # Combine question and result to generate a natural language answer
            prompt = f"é—®é¢˜ï¼š{question}\nSQLæŸ¥è¯¢ç»“æœï¼š{result}\nè¯·æ ¹æ®æŸ¥è¯¢ç»“æœç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆï¼ˆä»…ç»™å‡ºç­”æ¡ˆå³å¯ï¼Œæ— éœ€é‡å¤é—®é¢˜ï¼‰ï¼š"
            inputs = self.text_gen_tokenizer(prompt, return_tensors="pt").to(self.device)
            generated = self.text_gen_model.generate(
                inputs["input_ids"],
                max_new_tokens=150,
                early_stopping=True
            )
            answer = self.text_gen_tokenizer.decode(generated[0], skip_special_tokens=True)
            return answer.strip()
        except sqlite3.Error as e:
            print(f"[SQL Error] {str(e)}")
            # æŠ¥é”™æ—¶å­˜å‚¨é—®é¢˜ã€SQLå’Œé”™è¯¯ä¿¡æ¯
            with open('sql_log.json', 'a', encoding='utf-8') as log_file:
                json.dump(
                    {"question": question, "sql": sql, "error": str(e)},
                    log_file,
                    ensure_ascii=False,
                    indent=4
                )
                log_file.write("\n")
            return None

    def _init_vector_store(self, txt_dir, persist_dir):
        if os.path.exists(persist_dir):
            print(f"å‘é‡æ•°æ®åº“å·²å­˜åœ¨äº {persist_dir}ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return Chroma(persist_directory=persist_dir, embedding_function=self.embeddings)
        
        docs = []
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100
        )
        
        for filename in tqdm(os.listdir(txt_dir), desc="åŠ è½½æ–‡æ¡£"):
            if filename.endswith(".txt"):
                with open(os.path.join(txt_dir, filename), "r", encoding="utf-8") as f:
                    text = f.read()
                docs.extend(splitter.create_documents([text]))
        
        return Chroma.from_documents(
            documents=docs,
            embedding=self.embeddings,
            persist_directory=persist_dir
        )

    def classify_task(self, questions):
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä»è€Œä¸å¹²æ‰°question_vectorsçš„ç”Ÿæˆï¼Œè¿›è€Œä¸å½±å“ç›¸ä¼¼åº¦è®¡ç®—
        questions = ["".join(c for c in q if c not in "?!ï¼Ÿï¼ï¼Œã€‚") for q in questions]
        def determine_task_type(question):
            # ç¡®ä¿ç¼–ç æ­£ç¡®
            question = question.encode("utf-8").decode("utf-8")

            # ä½¿ç”¨ encode() + decode() æ–¹å¼è¿›è¡Œåˆ†è¯
            token_ids = self.text_gen_tokenizer.encode(question, add_special_tokens=False)
            # question_words = set(self.text_gen_tokenizer.decode([token]) for token in token_ids)
            '''
            åˆ†è¯ç»“æœç¤ºä¾‹ï¼š
                question_words: {'å¹´', 'åŸºé‡‘', 'å˜‰', '?', 'å®', 'åŸºé‡‘ç®¡ç†', 'æœ‰é™å…¬å¸', '0', '2', 'æˆç«‹äº†', '9', 'å¤šå°‘', '1'}
                question_words: {'æŠ€æœ¯', 'è‚¡ä»½', 'è´Ÿè´£', 'ç”Ÿç‰©', 'æœ‰é™å…¬å¸', 'ï¼Ÿ', 'äº§å“ç ”å‘', 'æ£®', 'æ²ƒ', 'äº‘å—', 'ä»€ä¹ˆ', 'éƒ¨é—¨', 'çš„æ˜¯'}
            '''
            # print("question_words:", question_words)

            match_count = sum(1 for char in question if any(char in keyword for keyword in self.db_keywords))
            if match_count / len(question) >= 0.3:
                return "data_query"
            else:
                docs = self.vector_store.similarity_search(question, k=1)
                return "text_comprehension" if docs else "other"

        return [determine_task_type(question) for question in questions]

    def structured_query(self, question):
        sql_prompt = self._generate_sql_prompt(question)
        # print('question:', question)
        # print('sql_prompt:', sql_prompt)
        inputs = self.sql_tokenizer(sql_prompt, return_tensors="pt").to(self.device)
        
        generated = self.sql_model.generate(
            inputs["input_ids"],
            max_new_tokens=500,
            num_return_sequences=3,
            early_stopping=True
        )
        sql = self.sql_tokenizer.decode(generated[0], skip_special_tokens=True)
        sql = sql.split("```sql\n")[-1].split(";")[0] + ";"
        # print('sql:', sql)
        
        # sql = self._sanitize_sql(sql)
        
        # Store the question and corresponding SQL in a file
        with open('question_to_sql.json', 'a', encoding='utf-8') as f:
            json.dump({"question": question, "sql": sql}, f, ensure_ascii=False, indent=4)
            f.write("\n")
        
        return self._execute_sql(question,sql)
        
    def rag_answer(self, questions):
        contexts = []
        for question in questions:
            docs = self.vector_store.similarity_search(question, k=3)
            context = "\n".join([d.page_content for d in docs])
            contexts.append(context)

        prompts = [f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
                    {context}

                    é—®é¢˜ï¼š{question}
                    ç­”æ¡ˆï¼š""" for context, question in zip(contexts, questions)]
        
        responses = self.text_gen_pipe(prompts, max_new_tokens=300, do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.2, stopping_criteria=stop_criteria_list)

        return [response[0]["generated_text"].split("ç­”æ¡ˆï¼š")[-1].strip() for response in responses]

    def process_batch(self, qid_batch, question_batch):
        task_types = self.classify_task(question_batch)
        answers = []
        
        with open('question_type.json', 'a', encoding='utf-8') as f:
            for task_type, question in zip(task_types, question_batch):
                json.dump({"question": question, "task_type": task_type}, f, ensure_ascii=False, indent=4)
                f.write("\n")
                answer = "No valid answer found"  # Default initialization
                if task_type == "data_query":
                    result = self.structured_query(question)
                    # print('question:', question)
                    # print('result:', result)
                    if result:
                        answer = " ".join([str(r) for r in result])
                else:
                    answer = self.rag_answer([question])[0]
                
                answers.append(answer)
        
        return [{"id": qid, "question": question, "answer": answer} for qid, question, answer in zip(qid_batch, question_batch, answers)]


class QuestionDataset(Dataset):
    def __init__(self, data_path):
        self.questions = []
        with open(data_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    self.questions.append(json.loads(line))

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        return self.questions[idx]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¸…é™¤question_to_sql.jsonä¸­çš„å†…å®¹
    with open('question_to_sql.json', 'w', encoding='utf-8') as f:
        f.write("")
    # æ¸…é™¤sql_log.jsonä¸­çš„å†…å®¹
    with open('sql_log.json', 'w', encoding='utf-8') as f:
        f.write("")
    # æ¸…é™¤question_type.jsonä¸­çš„å†…å®¹
    with open('question_type.json', 'w', encoding='utf-8') as f:
        f.write("")
    
    config = {
        # "text_gen_model": "models/Qwen2.5-7B-Instruct",
        # "text2sql_model": "models/Qwen2.5-7B-Instruct",
        "text_gen_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "text2sql_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "embed_model": "models/all-MiniLM-L6-v2",
        "db_path": "data/dataset/fund_data.db",
        "db_schema_path": "data/dataset/db_schema.json",
        "example_queries_path": "data/example_queries.txt",
        "txt_dir": "data/pdf_txt_file",
        "db_keywords_path": "data/dataset/db_keywords.txt",
        "device": "cuda:0" if torch.cuda.is_available() else "cpu"
    }
    
    qa_system = FinancialQA(config)
    
    data_path = "data/question_debug.json"
    # data_path = "data/question.json"
    dataset = QuestionDataset(data_path)
    dataloader = DataLoader(dataset, batch_size=5, shuffle=False)

    # ä½¿ç”¨æ‰¹é‡æ¨ç†
    results = []
    for batch in tqdm(dataloader, desc="Processing questions"):
        qid_batch = batch['id'].tolist()
        question_batch = batch['question']
        results.extend(qa_system.process_batch(qid_batch, question_batch))

    # Ensure all tensors are converted to lists before writing to JSON
    def tensor_to_list(obj):
        if isinstance(obj, torch.Tensor):
            return obj.tolist()
        if isinstance(obj, list):
            return [tensor_to_list(item) for item in obj]
        if isinstance(obj, dict):
            return {key: tensor_to_list(value) for key, value in obj.items()}
        return obj

    results = tensor_to_list(results)
    # print('results:', results)
    with open("submit_result.jsonl", "w", encoding="utf-8") as f:
        for res in results:
            f.write(json.dumps(res, ensure_ascii=False) + "\n")
    
    print(f"Processed {len(results)} questions.")
