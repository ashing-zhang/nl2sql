# DeepSpeed QLoRA Training

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼ QLoRA (Quantized Low-Rank Adaptation) è®­ç»ƒçš„é¡¹ç›®ã€‚

## é¡¹ç›®ç‰¹æ€§

- ğŸš€ ä½¿ç”¨ DeepSpeed ZeRO-3 è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- ğŸ”§ æ”¯æŒ QLoRA é‡åŒ–è®­ç»ƒï¼Œå¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚
- ğŸ“Š æ”¯æŒå¤šGPUè®­ç»ƒï¼Œæé«˜è®­ç»ƒæ•ˆç‡
- ğŸ¯ æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„ (LLaMA, Mistral, Qwenç­‰)
- ğŸ“ˆ å®Œæ•´çš„è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•
- ğŸ› ï¸ æ˜“äºé…ç½®å’Œæ‰©å±•

## é¡¹ç›®ç»“æ„

```
deepspeed_train/
â”œâ”€â”€ configs/                 # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ deepspeed_config.json
â”‚   â””â”€â”€ train_config.yaml
â”œâ”€â”€ data/                    # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ sample_data.json
â”œâ”€â”€ models/                  # æ¨¡å‹ç›¸å…³
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                 # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ utils/                   # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â””â”€â”€ model_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

å°†æ‚¨çš„è®­ç»ƒæ•°æ®æ”¾åœ¨ `data/` ç›®å½•ä¸‹ï¼Œæ ¼å¼ä¸º JSON æ–‡ä»¶ï¼š

```json
[
    {
        "instruction": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "input": "",
        "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
    }
]
```

### 2. é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ `configs/train_config.yaml` æ–‡ä»¶ï¼š

```yaml
model:
  base_model: "microsoft/DialoGPT-medium"
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1

training:
  num_epochs: 3
  batch_size: 4
  learning_rate: 2e-4
  max_length: 512

data:
  train_file: "data/sample_data.json"
  validation_split: 0.1
```

### 3. å¼€å§‹è®­ç»ƒ

```bash
# å•GPUè®­ç»ƒ
python scripts/train.py

# å¤šGPUè®­ç»ƒ
deepspeed --num_gpus=2 scripts/train.py --deepspeed configs/deepspeed_config.json
```

## é…ç½®è¯´æ˜

### DeepSpeed é…ç½®

`configs/deepspeed_config.json` åŒ…å« DeepSpeed çš„é…ç½®ï¼š

- **ZeRO-3**: å¯ç”¨ ZeRO-3 ä¼˜åŒ–
- **Gradient Accumulation**: æ”¯æŒæ¢¯åº¦ç´¯ç§¯
- **Mixed Precision**: ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- **Offload**: æ”¯æŒ CPU å’Œ NVMe å¸è½½

### è®­ç»ƒé…ç½®

`configs/train_config.yaml` åŒ…å«æ¨¡å‹å’Œè®­ç»ƒå‚æ•°ï¼š

- **æ¨¡å‹é…ç½®**: åŸºç¡€æ¨¡å‹ã€LoRA å‚æ•°
- **è®­ç»ƒé…ç½®**: æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°
- **æ•°æ®é…ç½®**: æ•°æ®æ–‡ä»¶è·¯å¾„ã€éªŒè¯é›†æ¯”ä¾‹

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ¨¡å‹

åœ¨ `models/` ç›®å½•ä¸‹æ·»åŠ æ‚¨çš„è‡ªå®šä¹‰æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

def create_model(base_model_name, lora_config):
    model = AutoModelForCausalLM.from_pretrained(base_model_name)
    peft_config = LoraConfig(**lora_config)
    model = get_peft_model(model, peft_config)
    return model
```

### è‡ªå®šä¹‰æ•°æ®é›†

åœ¨ `utils/data_utils.py` ä¸­æ·»åŠ æ‚¨çš„æ•°æ®å¤„ç†é€»è¾‘ï¼š

```python
def load_custom_dataset(file_path):
    # å®ç°æ‚¨çš„æ•°æ®åŠ è½½é€»è¾‘
    pass
```

## æ€§èƒ½ä¼˜åŒ–

### æ˜¾å­˜ä¼˜åŒ–

- ä½¿ç”¨ QLoRA é‡åŒ–ï¼Œå¯å‡å°‘ 70% æ˜¾å­˜ä½¿ç”¨
- å¯ç”¨ DeepSpeed ZeRO-3ï¼Œæ”¯æŒå¤§æ¨¡å‹è®­ç»ƒ
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient checkpointing)

### è®­ç»ƒåŠ é€Ÿ

- æ··åˆç²¾åº¦è®­ç»ƒ (FP16/BF16)
- æ¢¯åº¦ç´¯ç§¯
- å¤šGPU å¹¶è¡Œè®­ç»ƒ

## ç›‘æ§å’Œæ—¥å¿—

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆä»¥ä¸‹æ—¥å¿—ï¼š

- **è®­ç»ƒæŸå¤±**: å®æ—¶æ˜¾ç¤ºè®­ç»ƒå’ŒéªŒè¯æŸå¤±
- **å­¦ä¹ ç‡**: å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿
- **æ˜¾å­˜ä½¿ç”¨**: GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
- **è®­ç»ƒé€Ÿåº¦**: æ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ˜¾å­˜ä¸è¶³**: å‡å° batch_size æˆ–å¯ç”¨æ›´å¤šå¸è½½é€‰é¡¹
2. **è®­ç»ƒé€Ÿåº¦æ…¢**: æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒ
3. **æ”¶æ•›é—®é¢˜**: è°ƒæ•´å­¦ä¹ ç‡å’Œ LoRA å‚æ•°

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export TORCH_DISTRIBUTED_DEBUG=DETAIL
python scripts/train.py --debug
```

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è®¸å¯è¯

MIT License 